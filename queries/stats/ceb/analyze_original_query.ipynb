{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ad1197-343e-4dcb-96ce-a060859840a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2909cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/nobackup/yunjia/adaptiveness_vs_learning/runtime_eval/'\n",
    "import sys\n",
    "sys.path.append(PATH)\n",
    "# from core.eval_Runtime import *\n",
    "from core.QueryEvaluator import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fd373ca-c6ac-41d7-9117-5e796c4b4307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_needed_query_files = []\n",
    "stats_rand_queries_dir = './stats_rand/'\n",
    "for q in os.listdir(stats_rand_queries_dir):\n",
    "    if q.endswith('.sql'):\n",
    "        all_needed_query_files.append(q)\n",
    "\n",
    "stats_slow_queries_dir = './stats_slow/'\n",
    "for q in os.listdir(stats_slow_queries_dir):\n",
    "    if q.endswith('.sql'):\n",
    "        all_needed_query_files.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "163915be-bf00-483a-9bbd-87ea1187bb57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "def sql_formater(sql):\n",
    "    return sqlparse.format(sql, reindent=True, keyword_case='upper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8728a-3e8d-407d-8bfc-0bfd60bc4104",
   "metadata": {},
   "source": [
    "# Evaluate CEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a09f9159-cf81-473f-b866-f928b00eeffa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running queries:  ['5.sql', '44.sql', '119.sql', '38.sql', '69.sql', '17.sql', '23.sql', '25.sql', '107.sql', '83.sql', '113.sql', '129.sql', '78.sql', '101.sql', '135.sql', '67.sql', '145.sql', '87.sql', '26.sql', '142.sql', '118.sql', '7.sql', '3.sql', '94.sql', '37.sql', '116.sql', '128.sql', '99.sql', '103.sql', '38.sql', '56.sql', '69.sql', '115.sql', '141.sql', '58.sql', '51.sql', '107.sql', '52.sql', '136.sql', '138.sql', '70.sql', '68.sql', '123.sql', '108.sql', '135.sql', '134.sql', '30.sql', '140.sql', '143.sql', '126.sql', '122.sql', '49.sql', '142.sql', '105.sql', '71.sql', '59.sql', '120.sql', '34.sql']\n",
      "Set statment timetout to [('20min',)]\n",
      "Connected to the database successfully.\n",
      "Using ml_cardest_fname =  [('stats_CEB_sub_queries_bayescard.txt',)]\n",
      "Using ml_joinest_fname =  [('stats_CEB_sub_queries_bayescard.txt',)]\n",
      "Running query 101.sql (0 / 58)...\n",
      "\tQuery 101.sql has already been evaluated. Skipping...\n",
      "Running query 103.sql (1 / 58)...\n",
      "\tQuery 103.sql has already been evaluated. Skipping...\n",
      "Running query 105.sql (2 / 58)...\n",
      "Set statment timetout to [('20min',)]\n",
      "Connected to the database successfully.\n",
      "Using ml_cardest_fname =  [('stats_CEB_sub_queries_deepdb.txt',)]\n",
      "Using ml_joinest_fname =  [('stats_CEB_sub_queries_deepdb.txt',)]\n",
      "Running query 101.sql (0 / 58)...\n",
      "Set statment timetout to [('20min',)]\n",
      "Connected to the database successfully.\n",
      "Using ml_cardest_fname =  [('stats_CEB_sub_queries_flat.txt',)]\n",
      "Using ml_joinest_fname =  [('stats_CEB_sub_queries_flat.txt',)]\n",
      "Running query 101.sql (0 / 58)...\n",
      "Set statment timetout to [('20min',)]\n",
      "Connected to the database successfully.\n",
      "Using ml_cardest_fname =  [('stats_CEB_sub_queries_neurocard.txt',)]\n",
      "Using ml_joinest_fname =  [('stats_CEB_sub_queries_neurocard.txt',)]\n",
      "Running query 101.sql (0 / 58)...\n",
      "\tQuery 101.sql executed successfully in 0.71 seconds.\n",
      "\tQuery 101.sql executed successfully in 1.18 seconds.\n",
      "\tQuery 101.sql executed successfully in 1.21 seconds.\n",
      "\tQuery 101.sql executed successfully in 1.16 seconds.\n",
      "Running query 103.sql (1 / 58)...\n",
      "\tQuery 103.sql executed successfully in 0.99 seconds.\n",
      "\tQuery 101.sql executed successfully in 2.10 seconds.\n",
      "Running query 103.sql (1 / 58)...\n",
      "\tQuery 103.sql executed successfully in 0.58 seconds.\n",
      "Running query 105.sql (2 / 58)...\n",
      "\tQuery 103.sql executed successfully in 0.45 seconds.\n",
      "\tQuery 101.sql executed successfully in 2.87 seconds.\n",
      "Running query 103.sql (1 / 58)...\n",
      "\tQuery 103.sql executed successfully in 3.11 seconds.\n",
      "Running query 105.sql (2 / 58)...\n",
      "\tQuery 103.sql executed successfully in 3.72 seconds.\n",
      "\tError executing query 105.sql: server closed the connection unexpectedly\n",
      "\tThis probably means the server terminated abnormally\n",
      "\tbefore or while processing the request.\n",
      "\n",
      "\tTreating as a timeout.\n",
      "\tError executing query 105.sql: server closed the connection unexpectedly\n",
      "\tThis probably means the server terminated abnormally\n",
      "\tbefore or while processing the request.\n",
      "\n",
      "\tTreating as a timeout.\n",
      "\tError executing query 105.sql: server closed the connection unexpectedly\n",
      "\tThis probably means the server terminated abnormally\n",
      "\tbefore or while processing the request.\n",
      "\n",
      "\tTreating as a timeout.\n"
     ]
    },
    {
     "ename": "InterfaceError",
     "evalue": "connection already closed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/nobackup/yunjia/adaptiveness_vs_learning/runtime_eval/core/QueryEvaluator.py\", line 117, in evaluate_queries\n    self.cursor.execute(query)\npsycopg2.OperationalError: server closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/u/y/u/yunjia/anaconda3/envs/DataDisco/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/u/y/u/yunjia/anaconda3/envs/DataDisco/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/u/y/u/yunjia/anaconda3/envs/DataDisco/lib/python3.9/site-packages/joblib/parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n  File \"/u/y/u/yunjia/anaconda3/envs/DataDisco/lib/python3.9/site-packages/joblib/parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/tmp/ipykernel_3495233/788973061.py\", line 30, in eval_CEB\n  File \"/nobackup/yunjia/adaptiveness_vs_learning/runtime_eval/core/QueryEvaluator.py\", line 192, in run\n    self.evaluate_queries(\n  File \"/nobackup/yunjia/adaptiveness_vs_learning/runtime_eval/core/QueryEvaluator.py\", line 122, in evaluate_queries\n    self.conn.rollback()\npsycopg2.InterfaceError: connection already closed\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# parallel run of all ceb files\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[0;32m---> 38\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_CEB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mceb_file_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mceb_file_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstats_CEB_sub_queries_bayescard.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstats_CEB_sub_queries_deepdb.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstats_CEB_sub_queries_flat.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstats_CEB_sub_queries_neurocard.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DataDisco/lib/python3.9/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DataDisco/lib/python3.9/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DataDisco/lib/python3.9/site-packages/joblib/parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1693\u001b[0m \n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DataDisco/lib/python3.9/site-packages/joblib/parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DataDisco/lib/python3.9/site-packages/joblib/parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DataDisco/lib/python3.9/site-packages/joblib/parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mInterfaceError\u001b[0m: connection already closed"
     ]
    }
   ],
   "source": [
    "db_config = {\n",
    "    'dbname': 'stats',\n",
    "    'user': 'yunjia',\n",
    "    'password': 'yunjia',\n",
    "    'host': 'localhost',\n",
    "    'port': '5433'\n",
    "}\n",
    "\n",
    "query_directory = './all/'\n",
    "print(\"Running queries: \", all_needed_query_files)\n",
    "\n",
    "\n",
    "\n",
    "# for ceb_file_name in [\n",
    "#     'stats_CEB_sub_queries_bayescard.txt',\n",
    "#     'stats_CEB_sub_queries_deepdb.txt',\n",
    "#     'stats_CEB_sub_queries_flat.txt',\n",
    "#     'stats_CEB_sub_queries_neurocard.txt'\n",
    "# ]:\n",
    "\n",
    "def eval_CEB(ceb_file_name):\n",
    "    evaluator = PostgresCEBQueryEvaluator(\n",
    "        db_config, \n",
    "        query_directory,\n",
    "        query_files=all_needed_query_files,                                   \n",
    "        debug_mode=False, \n",
    "        ceb_file_name=ceb_file_name\n",
    "    )\n",
    "\n",
    "    evaluator.run(\n",
    "        query_log_file = f'ceb-stats-{ceb_file_name.split(\".\")[0]}.json',\n",
    "        rerun_finished=False,\n",
    "        sample_size=None\n",
    "    )\n",
    "\n",
    "# parallel run of all ceb files\n",
    "from joblib import Parallel, delayed\n",
    "Parallel(n_jobs=4)(delayed(eval_CEB)(ceb_file_name) for ceb_file_name in [\n",
    "    'stats_CEB_sub_queries_bayescard.txt',\n",
    "    'stats_CEB_sub_queries_deepdb.txt',\n",
    "    'stats_CEB_sub_queries_flat.txt',\n",
    "    'stats_CEB_sub_queries_neurocard.txt'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a52d6ef-0fa1-4673-9c05-19d7dfda3860",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using log:  stats_CEB_sub_queries_bayescard.txt\n",
      "Stats rand total time:  1699.9131639003754\n",
      "Stats slow total time:  9266.137593507767\n",
      "Using log:  stats_CEB_sub_queries_deepdb.txt\n",
      "Stats rand total time:  498.4509971141815\n",
      "Stats slow total time:  10429.272281646729\n",
      "Using log:  stats_CEB_sub_queries_flat.txt\n",
      "Missing:  5.sql\n",
      "Missing:  44.sql\n",
      "Missing:  38.sql\n",
      "Missing:  69.sql\n",
      "Missing:  17.sql\n",
      "Missing:  23.sql\n",
      "Missing:  25.sql\n",
      "Missing:  83.sql\n",
      "Missing:  129.sql\n",
      "Missing:  78.sql\n",
      "Missing:  135.sql\n",
      "Missing:  67.sql\n",
      "Missing:  145.sql\n",
      "Missing:  87.sql\n",
      "Missing:  26.sql\n",
      "Missing:  142.sql\n",
      "Missing:  7.sql\n",
      "Missing:  3.sql\n",
      "Missing:  94.sql\n",
      "Missing:  37.sql\n",
      "Missing:  128.sql\n",
      "Missing:  99.sql\n",
      "Stats rand total time:  408.3313593864441\n",
      "Missing:  38.sql\n",
      "Missing:  56.sql\n",
      "Missing:  69.sql\n",
      "Missing:  141.sql\n",
      "Missing:  58.sql\n",
      "Missing:  51.sql\n",
      "Missing:  52.sql\n",
      "Missing:  136.sql\n",
      "Missing:  138.sql\n",
      "Missing:  70.sql\n",
      "Missing:  68.sql\n",
      "Missing:  123.sql\n",
      "Missing:  135.sql\n",
      "Missing:  134.sql\n",
      "Missing:  30.sql\n",
      "Missing:  140.sql\n",
      "Missing:  143.sql\n",
      "Missing:  126.sql\n",
      "Missing:  49.sql\n",
      "Missing:  142.sql\n",
      "Missing:  71.sql\n",
      "Missing:  59.sql\n",
      "Missing:  34.sql\n",
      "Stats slow total time:  3267.0141637325287\n",
      "Using log:  stats_CEB_sub_queries_neurocard.txt\n",
      "Missing:  5.sql\n",
      "Missing:  44.sql\n",
      "Missing:  38.sql\n",
      "Missing:  69.sql\n",
      "Missing:  17.sql\n",
      "Missing:  23.sql\n",
      "Missing:  25.sql\n",
      "Missing:  83.sql\n",
      "Missing:  129.sql\n",
      "Missing:  78.sql\n",
      "Missing:  135.sql\n",
      "Missing:  67.sql\n",
      "Missing:  145.sql\n",
      "Missing:  87.sql\n",
      "Missing:  26.sql\n",
      "Missing:  142.sql\n",
      "Missing:  7.sql\n",
      "Missing:  3.sql\n",
      "Missing:  94.sql\n",
      "Missing:  37.sql\n",
      "Missing:  128.sql\n",
      "Missing:  99.sql\n",
      "Stats rand total time:  623.0072774887085\n",
      "Missing:  38.sql\n",
      "Missing:  56.sql\n",
      "Missing:  69.sql\n",
      "Missing:  141.sql\n",
      "Missing:  58.sql\n",
      "Missing:  51.sql\n",
      "Missing:  52.sql\n",
      "Missing:  136.sql\n",
      "Missing:  138.sql\n",
      "Missing:  70.sql\n",
      "Missing:  68.sql\n",
      "Missing:  123.sql\n",
      "Missing:  135.sql\n",
      "Missing:  134.sql\n",
      "Missing:  30.sql\n",
      "Missing:  140.sql\n",
      "Missing:  143.sql\n",
      "Missing:  126.sql\n",
      "Missing:  122.sql\n",
      "Missing:  49.sql\n",
      "Missing:  142.sql\n",
      "Missing:  71.sql\n",
      "Missing:  59.sql\n",
      "Missing:  120.sql\n",
      "Missing:  34.sql\n",
      "Stats slow total time:  1419.127671957016\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "all_needed_query_files = []\n",
    "stats_rand_queries_dir = './stats_rand/'\n",
    "for q in os.listdir(stats_rand_queries_dir):\n",
    "    if q.endswith('.sql'):\n",
    "        all_needed_query_files.append(q)\n",
    "\n",
    "stats_slow_queries_dir = './stats_slow/'\n",
    "for q in os.listdir(stats_slow_queries_dir):\n",
    "    if q.endswith('.sql'):\n",
    "        all_needed_query_files.append(q)\n",
    "\n",
    "for ceb_file_name in ['stats_CEB_sub_queries_bayescard.txt',\n",
    "    'stats_CEB_sub_queries_deepdb.txt',\n",
    "    'stats_CEB_sub_queries_flat.txt',\n",
    "    'stats_CEB_sub_queries_neurocard.txt']:\n",
    "    log_file = f'ceb-stats-{ceb_file_name.split(\".\")[0]}.json'\n",
    "    runtime_dict = json.load(open(log_file))\n",
    "\n",
    "    print(\"Using log: \", ceb_file_name)\n",
    "\n",
    "    total_time = 0\n",
    "    for q in [qfile for qfile in os.listdir('./stats_rand/') if qfile.endswith('.sql')]:\n",
    "        if q not in runtime_dict:\n",
    "            print(\"Missing: \", q)\n",
    "            continue\n",
    "        total_time += min(runtime_dict[q])\n",
    "    print(\"Stats rand total time: \", total_time)\n",
    "\n",
    "    total_time = 0\n",
    "    for q in [qfile for qfile in os.listdir('./stats_slow/') if qfile.endswith('.sql')]:\n",
    "        if q not in runtime_dict:\n",
    "            print(\"Missing: \", q)\n",
    "            continue\n",
    "        total_time += min(runtime_dict[q])\n",
    "    print(\"Stats slow total time: \", total_time)\n",
    "\n",
    "# total_time = 0\n",
    "# for q in random_queries:\n",
    "#     total_time += sorted_dict[q]\n",
    "# print(\"Stats rand total time: \", total_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562724e-9d89-4f2c-b6a6-541f5f712007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "runtime_stats = json.load(open(f'ceb-stats-stats_CEB_sub_queries_deepdb-non-parallel'))\n",
    "all_workload_runtime = 0\n",
    "for q in runtime_stats:\n",
    "    all_workload_runtime += np.average(runtime_stats[q])\n",
    "print(\"All workload runtime for deepdb non-parallel: \", all_workload_runtime)\n",
    "\n",
    "runtime_stats = json.load(open(f'ceb-stats-stats_CEB_sub_queries_deepdb-non-parallel'))\n",
    "all_workload_runtime = 0\n",
    "for q in runtime_stats:\n",
    "    all_workload_runtime += np.average(runtime_stats[q])\n",
    "print(\"All workload runtime: \", all_workload_runtime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
